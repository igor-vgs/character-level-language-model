{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python376jvsc74a57bd039987a4d2379493807553e497db7790592d8ba2ae8bf6bbc41ff07c0835f77c4",
   "display_name": "Python 3.7.6 64-bit ('pytorch_env': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "import torch\n",
    "\n",
    "from src.tokenizer import Tokenizer\n",
    "from src.dataset import WordsDataset\n",
    "from src.model import CharLanguageModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Build vocabulary\n",
      "Vocab size: 270\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "source": [
    "data = WordsDataset._read_data('./data/wikitext-2/wiki.train.tokens')\n",
    "tokenizer = Tokenizer(data)\n",
    "\n",
    "model = CharLanguageModel(100, 50, tokenizer.vocab_size, tokenizer.pad_id)\n",
    "model.eval()\n",
    "\n",
    "ckpt = glob.glob('./checkpoints/*.ckpt')[0]\n",
    "model.load_state_dict(torch.load(ckpt, map_location=torch.device('cpu')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(model, start_chars, tokenizer, max_length=20, top_k=100):\n",
    "    encoded = tokenizer.encode(start_chars)['data']\n",
    "    generated_seq = encoded\n",
    "    encoded = torch.LongTensor([encoded,])\n",
    "    while len(generated_seq) < max_length and generated_seq[-1] != tokenizer.eos_id:\n",
    "        with torch.no_grad():\n",
    "            next_token_prob = model(encoded)[0, -1, :]\n",
    "        \n",
    "        logits, inds = next_token_prob.topk(top_k)\n",
    "        logits = torch.softmax(logits, dim=-1)\n",
    "        new_ind = torch.multinomial(logits, 1)\n",
    "        token_ind = inds[new_ind]\n",
    "        encoded = torch.cat([encoded, token_ind.unsqueeze(0)], dim=-1)\n",
    "        token_ind = token_ind.cpu().item()\n",
    "        generated_seq.append(token_ind)\n",
    "    return ' '.join(tokenizer.decode(generated_seq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'H e r t r r o m'"
      ]
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "source": [
    "generate(model, 'He', tokenizer)"
   ]
  }
 ]
}